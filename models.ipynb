{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackOverflow assistant bot\n",
    "\n",
    "This project combine different concepts learned about [Natural Language Processing](https://www.coursera.org/learn/language-processing) into a simple dialog chatbot capable of:\n",
    "\n",
    "* Answering user software-programming-related questions (using StackOverflow dataset);\n",
    "* Chit-chatting and simulating a dialogue on all non-programming-related questions.\n",
    "\n",
    "The chit-chat mode uses a pre-trained Neural Network Engine available from [ChatterBot](https://chatterbot.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project description\n",
    "\n",
    "The ChatBot will be constantly wating for user input, either a software development related question or more generic dialog input.\n",
    "\n",
    "The intent of the user is identified using a classifier model trained on the following datasates:\n",
    "\n",
    "1. `data/tagged_posts.tsv` — StackOverflow posts, tagged with one programming language (*positive samples*).\n",
    "2.  `data/dialogues.tsv` — dialogue phrases from movie subtitles (*negative samples*)\n",
    "\n",
    "Software-related questions will be further classified depending from the relevant programming language (C/C++, Python, Java, etc.) and ranked according to the probability of each classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/alberto/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Intent and language recognition\n",
    "\n",
    "The goal of this project is to write a ChatBot capable to **maintain an entertaining dialogue** with the user as well as to **understand and answer software programming-related questions**. \n",
    "\n",
    "In case of a programming-related question, the Bot should also be able to understand which programming language the user is referring. This step will help in speeding up the search for the most appropriate answer in our StackOverflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Transform text into features using a TF-IDF transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test, vectorizer_path):\n",
    "    \"\"\"\n",
    "    Performs TF-IDF transformation and dumps the model.\n",
    "    \n",
    "    - Train a vectorizer on X_train data.\n",
    "    - Transform X_train and X_test data.\n",
    "    - Pickle the trained vectorizer to 'vectorizer_path'\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        encoding='utf-8', \n",
    "        min_df=5, max_df=0.9, \n",
    "        ngram_range=(1,2), \n",
    "        token_pattern='(\\S+)')\n",
    "        \n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train = tfidf_vectorizer.transform(X_train)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    output = open(vectorizer_path, 'wb')\n",
    "    joblib.dump(tfidf_vectorizer, output)\n",
    "    output.close()\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load examples of two classes,  using a subsample of stackoverflow data to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200000\n",
    "\n",
    "dialogue_df = pd.read_csv('data/dialogues.tsv', sep='\\t').sample(sample_size, random_state=0)\n",
    "\n",
    "stackoverflow_df = pd.read_csv('data/tagged_posts.tsv', sep='\\t').sample(sample_size, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                     text       tag\n82925                            Donna, you are a muffin.  dialogue\n48774   He was here last night till about two o'clock....  dialogue\n55394   All right, then make an appointment with her s...  dialogue\n90806   Hey, what is this-an interview? We're supposed...  dialogue\n107758  Yeah. He's just a friend of mine I was trying ...  dialogue",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>82925</th>\n      <td>Donna, you are a muffin.</td>\n      <td>dialogue</td>\n    </tr>\n    <tr>\n      <th>48774</th>\n      <td>He was here last night till about two o'clock....</td>\n      <td>dialogue</td>\n    </tr>\n    <tr>\n      <th>55394</th>\n      <td>All right, then make an appointment with her s...</td>\n      <td>dialogue</td>\n    </tr>\n    <tr>\n      <th>90806</th>\n      <td>Hey, what is this-an interview? We're supposed...</td>\n      <td>dialogue</td>\n    </tr>\n    <tr>\n      <th>107758</th>\n      <td>Yeah. He's just a friend of mine I was trying ...</td>\n      <td>dialogue</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dialogue_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          post_id                                              title  \\\n2168983  43837842  Efficient Algorithm to compose valid expressio...   \n1084095  15747223  Why does this basic thread program fail with C...   \n1049020  15189594                  Link to scroll to top not working   \n200466    3273927  Is it possible to implement ping on windows ph...   \n1200249  17684551                          GLSL normal mapping issue   \n\n                tag  \n2168983      python  \n1084095       c_cpp  \n1049020  javascript  \n200466           c#  \n1200249       c_cpp  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post_id</th>\n      <th>title</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2168983</th>\n      <td>43837842</td>\n      <td>Efficient Algorithm to compose valid expressio...</td>\n      <td>python</td>\n    </tr>\n    <tr>\n      <th>1084095</th>\n      <td>15747223</td>\n      <td>Why does this basic thread program fail with C...</td>\n      <td>c_cpp</td>\n    </tr>\n    <tr>\n      <th>1049020</th>\n      <td>15189594</td>\n      <td>Link to scroll to top not working</td>\n      <td>javascript</td>\n    </tr>\n    <tr>\n      <th>200466</th>\n      <td>3273927</td>\n      <td>Is it possible to implement ping on windows ph...</td>\n      <td>c#</td>\n    </tr>\n    <tr>\n      <th>1200249</th>\n      <td>17684551</td>\n      <td>GLSL normal mapping issue</td>\n      <td>c_cpp</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "stackoverflow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply *text_prepare* function to preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import text_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df['text'] = [text_prepare(text) for text in dialogue_df['text']]\n",
    "\n",
    "stackoverflow_df['title'] = [text_prepare(title) for title in stackoverflow_df['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent recognition\n",
    "\n",
    "Implement a binary classification on TF-IDF representations of texts. \n",
    "\n",
    "Labels will be either `dialogue` for general questions or `stackoverflow` for programming-related questions. \n",
    "\n",
    "First, prepare the data for this task:\n",
    "* concatenate `dialogue` and `stackoverflow` examples into one sample\n",
    "* split it into train and test \n",
    "* transform it into TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train size = 360000, test size = 40000\n"
    }
   ],
   "source": [
    "X = np.concatenate([dialogue_df['text'].values, stackoverflow_df['title'].values])\n",
    "y = ['dialogue'] * dialogue_df.shape[0] + ['stackoverflow'] * stackoverflow_df.shape[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test, RESOURCE_PATH['TFIDF_VECTORIZER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the **intent recognizer** using LogisticRegression on the train set. \n",
    "\n",
    "Use the following parameters:\n",
    "\n",
    "1. *penalty='l2'*, \n",
    "2. *C=10*, \n",
    "3. *random_state=0*. \n",
    "\n",
    "Print out the accuracy on the test set to check whether everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "intent_recognizer = LogisticRegression(penalty='l2', C=10, random_state=0)\n",
    "intent_recognizer.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test accuracy = 0.99185\n"
    }
   ],
   "source": [
    "# Check test accuracy.\n",
    "y_test_pred = intent_recognizer.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy = {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the classifier to use it in the running bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(intent_recognizer, open(RESOURCE_PATH['INTENT_RECOGNIZER'], 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming language classification\n",
    "\n",
    "Implement a multi class logistic regression on the TF-IDF representations of texts in orders to classify the user programming-related question into the relevant programming language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stackoverflow_df['title'].values\n",
    "y = stackoverflow_df['tag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train size = 160000, test size = 40000\n"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reuse the TF-IDF vectorizer already created above. It should not make a huge difference which data was used to train it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load(open(RESOURCE_PATH['TFIDF_VECTORIZER'], 'rb'))\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the **tag classifier** using OneVsRestClassifier wrapper over LogisticRegression. \n",
    "\n",
    "Use the following parameters: \n",
    "\n",
    "1. *penalty='l2'*, \n",
    "2. *C=5*, \n",
    "3. *random_state=0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OneVsRestClassifier(estimator=LogisticRegression(C=5, class_weight=None,\n                                                 dual=False, fit_intercept=True,\n                                                 intercept_scaling=1,\n                                                 l1_ratio=None, max_iter=100,\n                                                 multi_class='auto',\n                                                 n_jobs=None, penalty='l2',\n                                                 random_state=0, solver='lbfgs',\n                                                 tol=0.0001, verbose=0,\n                                                 warm_start=False),\n                    n_jobs=None)"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "tag_classifier = OneVsRestClassifier(\n",
    "    LogisticRegression(penalty='l2', C=5, random_state=0)\n",
    ")\n",
    "\n",
    "tag_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test accuracy = 0.80055\n"
    }
   ],
   "source": [
    "# Check test accuracy.\n",
    "y_test_pred = tag_classifier.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy = {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the classifier to use it in the running bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tag_classifier, open(RESOURCE_PATH['TAG_CLASSIFIER'], 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Ranking questions with embeddings\n",
    "\n",
    "Vectorized representation of the text (embeddings) are used to find relevant answers to user questions by calculating the similarity with respect to existing thread in our StackOverflow dataset.\n",
    "\n",
    "Since it would be computationally expensive to calculate such a representation for all possible answers in *online mode* (e.g. when the Bot is running and answering questions from many users), answers vector representation will be pre-computed and stored on a *databse*. Pre-computed representation will be arranged by non-overlaping tags (i.e. programming languages), so that the search of the answer can be performed only within one tag each time. This will make the Bot even more efficient and it will allow not to store all the embedding database in memory. \n",
    "\n",
    "The following code employ StarSpace embeddings pre-trained in *supervised mode* on Stack Overflow posts. Alternatively, [pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google could also be employed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings, embeddings_dim = load_embeddings('data/ss_embeddings.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "95058"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "len(starspace_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the whole post dataset in order to pre-compute the embeddings on all possible answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('data/tagged_posts.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of posts for programming languages (tags) and find the most common ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_tag = posts_df.groupby('tag').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tag\nc#            394451\nc_cpp         281300\njava          383456\njavascript    375867\nphp           321752\npython        208607\nr              36359\nruby           99930\nswift          34809\nvb             35044\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "counts_by_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two data structures for each `tag` which will serve as online search index:\n",
    "\n",
    "* `tag_post_ids` — a list of post_ids with shape `(counts_by_tag[tag],)`. It will be needed to show the title and link to the thread;\n",
    "* `tag_vectors` — a matrix with shape `(counts_by_tag[tag], embeddings_dim)` where embeddings for each answer are stored.\n",
    "\n",
    "Implement the code which will calculate the mentioned structures and dump it to files. It should take several minutes to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(RESOURCE_PATH['THREAD_EMBEDDINGS_FOLDER'], exist_ok=True)\n",
    "\n",
    "for tag, count in counts_by_tag.items():\n",
    "        \n",
    "    tag_posts = posts_df[posts_df['tag'] == tag]\n",
    "    \n",
    "    tag_post_ids = tag_posts['post_id'].tolist()\n",
    "    \n",
    "    tag_vectors = np.zeros((count, embeddings_dim), dtype=np.float32)\n",
    "    \n",
    "    for i, title in enumerate(tag_posts['title']):\n",
    "        tag_vectors[i, :] = question_to_vec(title, starspace_embeddings, embeddings_dim)\n",
    "\n",
    "    # Dump post ids and vectors to a file.\n",
    "    filename = os.path.join(RESOURCE_PATH['THREAD_EMBEDDINGS_FOLDER'], os.path.normpath('%s.pkl' % tag))\n",
    "    joblib.dump((tag_post_ids, tag_vectors), open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenva62e06df51954ca48da2f148ea47f1c5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}